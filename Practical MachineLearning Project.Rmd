---
title: "Practical Machine Learning Project"
author: "Senthil"
date: "25 July 2015"
output: html_document
---

Background:


Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement - a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. In this project, your goal will be to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways. More information is available from the website here: http://groupware.les.inf.puc-rio.br/har (see the section on the Weight Lifting Exercise Dataset). 

Six young health participants were asked to perform one set of 10 repetitions of the Unilateral Dumbbell Biceps Curl in five different fashions: exactly according to the specification (Class A), throwing the elbows to the front (Class B), lifting the dumbbell only halfway (Class C), lowering the dumbbell only halfway (Class D) and throwing the hips to the front (Class E).

Read more: http://groupware.les.inf.puc-rio.br/har#ixzz3gpuuDA00

Class A corresponds to the specified execution of the exercise, while the other 4 classes correspond to common mistakes. 

Participants were supervised by an experienced weight lifter to make sure the execution complied to the manner they were supposed to simulate. The exercises were performed by six male participants aged between 20-28 years, with little weight lifting experience. We made sure that all participants could easily simulate the mistakes in a safe and controlled manner by using a relatively light dumbbell (1.25kg).

Read more: http://groupware.les.inf.puc-rio.br/har#ixzz3gpv00eUD


Objective:
Predict the manner in which they did the exercise.(i.e) Classe = 'A'- Exactly according to Specification or common mistakes - 'B' throw elbow to the front, 'C' lifting the dumbbell only halfway, 'D' lowering the dumbbell only halfway and 'E' throwing the hips to the front  

Data :

The training data for this project are available here: 

https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv

The test data are available here: 

https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv

The data for this project come from this source: http://groupware.les.inf.puc-rio.br/har. 



```{r}
trainingdata<- read.csv(url("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"))
testingdata <-read.csv(url("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"))

```


Explorative Data Analysis
  With the Summary and Head command got to know total 160-1 variables (predictors) and 19622 numbers of instances
  With the quick snap at summary(trainingdata) output could see lots of NA values and un related variables like timestamp,divby zero. With the na.remove function removed all NA valuesDIM
  
  ```{r}
  traindatacleaned<- na.omit(trainingdata)
  dim(traindatacleaned)
  
  ```
after omitting NA vlaues got 406 rows and 160 variables.

Note: actually i tried to train the model without preprocess with the intial training set (70%) but the it was running continuously and no putput even after 1 hr. its clearly shows the significance of data preprocessing

```{r}
library(caret)
 #Remove near Zero Variance columns
traindatacleaned1 <-traindatacleaned[,-nearZeroVar(traindatacleaned)]


 #Remove high corelated features
correlationMatrix<- abs(cor(traindatacleaned[, sapply(traindatacleaned, is.numeric)]))
traindatacleaned <-traindatacleaned[, -findCorrelation(correlationMatrix,cutoff=0.90)]
```
Model :
Splitted the training data to two data sets 75% training and 25% for test validation

```{r, echo=FALSE}
library(caret)
set.seed(17241)
inTrainIndex<- createDataPartition(y=traindatacleaned$classe,p=0.75,list=FALSE)
training <- traindatacleaned[inTrainIndex,]
testing <- traindatacleaned[-inTrainIndex,]


```
Tried to train the model with Linear regression, PCA and Random forest just to try all and conclude best model based on the accuracy. Since this problem need more accuracy chosen Random Forest since Lm just given 42%.

```{r, echo=FALSE}
# define training control to use 5-fold cross validation
train_control <- trainControl(method="cv", number=5)

FirstModel <- train (classe ~ ., data=training, method="rf", trControl=train_control)

summary(FirstModel)
FirstModel$finalModel
``` 
In sample error rate 1.3%. 

cross validation- Out of sample error
```{r, echo=FALSE}
predictionTest<-predict(FirstModel,newdata=testing)
confusionMatrix <-confusionMatrix(predictionTest,testing$classe)
confusionMatrix
```
You can also embed plots

```{r, echo=FALSE}
plot(FirstModel)
```

The following plot shows the resulting Accuracy as a function of the number of randomly selected predictors:

```{r, echo=FALSE}
plot(FirstModel, metric="Accuracy")
```
The following plot shows the relative importance of the top 25 variables in the final model:
```{r, echo=FALSE}
plot(varImp(FirstModel), top=25)
```

Final Prediction

Now actual prediction on Test data provided in this exercise . just take the actual features and stored as colnames and take those columns alone in testing data as actual test data as below
```{r, echo=FALSE}
selectedFeatureColumn<-colnames(training)
actualTestdata<-testingdata[,colnames(testingdata) %in% selectedFeatureColumn]

#FinalModelRf<- predict(FirstModel,newdata = actualTestdata)
```

unfortunately, predict function taking very long time and finally end up with some error message -almost after 3 hrs.
checked once again and suspect during data cleaning something got messed up since when i start predicting with acual Test data. 

So tried different approach since nearing project submission time.

Did data descriptive analysis once again and as already highlighted lots of variables has N/A and variables which may not be a predictor category like timestamp, stddev and etc. So took new trainingdata and Testingdata with following variales as a predictor

```{r, echo=FALSE}
trainingdataNew<-trainingdata[,c("classe","num_window","roll_belt","pitch_belt","yaw_belt","total_accel_belt","roll_arm","pitch_arm","yaw_arm","roll_dumbbell","pitch_dumbbell","yaw_dumbbell")]
head(trainingdataNew)
testingdataNew <-testingdata[,c("num_window","roll_belt","pitch_belt","yaw_belt","total_accel_belt","roll_arm","pitch_arm","yaw_arm","roll_dumbbell","pitch_dumbbell","yaw_dumbbell")]

dim(trainingdataNew);dim(testingdataNew)
```

Seed has been set as 17241 and Data slicing as 75%-25% training data 

```{r, echo=FALSE}
set.seed(17241)
inTrainIndex<- createDataPartition(y=trainingdataNew$classe,p=0.75,list=FALSE)
training <- trainingdataNew[inTrainIndex,]
testing <- trainingdataNew[-inTrainIndex,]
dim(training);dim(testing)
train_control <- trainControl(method="cv", number=5)

FirstModel <- train (classe ~ ., data=training, method="rf", trControl=train_control)

summary(FirstModel)
```
Cross validation using the 25% the test data and using confusion matrix confirmed no outfit and accuracy of the model built is satisfactory - got 99%. 
Now, with the actual test data (20 samples) get the final predict model and result are predicted perfect
```{r, echo=FALSE}
predictionTest<-predict(FirstModel,newdata=testing)
predictionTest
dim(testing)
confusionMatrix <-confusionMatrix(predictionTest,testing$classe)
confusionMatrix
plot(FirstModel, metric="Accuracy")

finalPredictedModel <- predict(FirstModel, newdata=testingdataNew)
finalPredictedModel
#plot(finalPredictedModel)
```
